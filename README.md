# ğŸ” Multi-document Embedding Search Engine (with Caching)

A lightweight semantic search engine that:

- Embeds multiple documents with **Sentence Transformers**
- Uses **content-hash caching** to avoid recomputing embeddings
- Builds a **FAISS index** for fast nearest-neighbor search
- Exposes both **Streamlit UI** and **FastAPI API**
- Provides simple and transparent **search explanations**

---

## ğŸ“¸ Preview

![Web App](Preview1.png)
![Web App](Preview2.png)

---

## ğŸš€ Live Demo

ğŸ”— **Live Streamlit App:**  
[https://multi-document-embedding-search-engine-with-caching.streamlit.app/]

---

## ğŸ“‚ Folder Structure
```
â”œâ”€â”€ api.py # FastAPI backend
â”œâ”€â”€ app.py # Streamlit UI
â”œâ”€â”€ embedder.py # Embedding + caching logic
â”œâ”€â”€ search_engine.py # FAISS index and search functions
â”œâ”€â”€ cache_manager.py # Handles hashing and embedding_cache.json
â”œâ”€â”€ embedding_cache.json # Auto-generated cache file
â”œâ”€â”€ data/
â”‚ â””â”€â”€ docs/ # Place your .txt documents here
â””â”€â”€ README.md
```

---

## âš™ï¸ How Caching Works

Caching is implemented using **SHA-256 hashes** of cleaned document content.

### ğŸ”„ Caching Process

1. Document text is cleaned (lowercasing, HTML removal, whitespace normalization).
2. A **SHA-256 hash** of the cleaned text is generated.
3. `embedding_cache.json` stores:
   - `doc_id`
   - `hash` (content hash)
   - `embedding`
   - `metadata` (filename, timestamp)
4. When loading documents:
   - If hash **matches** â†’ **reuse embedding**
   - If hash **differs** or absent â†’ **regenerate embedding**

### âœ… Why this method?

- Avoids re-running expensive embedding operations  
- Auto-invalidates when file content changes  
- JSON format is simple and version-friendly  
- Fast and predictable

### ğŸ”§ Forcing a regeneration

Delete the cache:

```bash
rm embedding_cache.json
```

# Multi-document Embedding Search Engine (with Caching)

## ğŸ§  How to Run Embedding Generation

The system generates embeddings in three ways.

### 1. Manual Generation (recommended for debugging)
```
python -c "from embedder import Embedder; Embedder().process_documents()"
```

### 2. Streamlit UI (auto generates embeddings)
```
streamlit run app.py
```

### 3. API Startup (auto generates embeddings)
```
uvicorn api:app --host 0.0.0.0 --port 8000
```

All three will:
- Read documents from `data/docs/`
- Check cache
- Generate missing embeddings
- Save the updated cache

---

## ğŸŒ How to Start the API

Start the FastAPI backend:
```
uvicorn api:app --host 0.0.0.0 --port 8000
```

### â¤ Endpoint: /search

POST request:
```json
{
  "query": "machine learning",
  "top_k": 5
}
```

Example CURL:
```
curl -X POST "http://localhost:8000/search"   -H "Content-Type: application/json"   -d '{"query": "deep learning", "top_k": 3}'
```

---

## ğŸ–¥ï¸ How to Start the Streamlit UI
```
streamlit run app.py
```

The UI:
- Loads embeddings (using caching)
- Builds FAISS index
- Lets you run semantic search interactively

---

## ğŸ§± Design Choices

### 1. SHA-256 Content Hashing for Caching
- Guarantees accurate invalidation
- Simple and deterministic
- Avoids timestamp inconsistencies

### 2. Sentence Transformer Model
Default: `all-MiniLM-L6-v2`
- Fast on CPU
- Memory-efficient
- High semantic quality

### 3. FAISS: IndexFlatIP
- Cosine similarity (with normalized vectors)
- Extremely fast nearest-neighbor search
- Scalable and flexible

### 4. Search Explanation
- Token overlap
- Overlap ratio
- Human-readable 'why this result'

### 5. Modular Architecture
- Embedder â†’ preprocessing + embedding + caching
- CacheManager â†’ load/save cache
- SearchEngine â†’ FAISS search + explanations
- api.py â†’ backend
- app.py â†’ UI

---

## ğŸ“¦ Requirements
```
sentence-transformers
faiss-cpu
numpy
fastapi
uvicorn
pydantic
streamlit
```

---

## ğŸ›  Troubleshooting

| Problem | Solution |
|--------|----------|
| Embeddings not updating | Delete `embedding_cache.json` |
| No documents loaded | Add `.txt` files in `data/docs/` |
| Slow embedding generation | Use GPU or batching |
| FAISS index empty | Ensure `process_documents()` ran |

---

## ğŸš€ Optional Improvements
- Document chunking
- Hybrid BM25 + Vector search
- GPU FAISS
- Docker support
- Redis/SQLite cache backend
